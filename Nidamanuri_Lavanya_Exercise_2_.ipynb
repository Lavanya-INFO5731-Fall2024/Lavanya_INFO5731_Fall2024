{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lavanya-INFO5731-Fall2024/Lavanya_INFO5731_Fall2024/blob/main/Nidamanuri_Lavanya_Exercise_2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Research Question:**\n",
        "**\"How does population size correlate with key demographic indicators such as growth rate, population density, and urban population percentage across different countries?\"**\n",
        "\n",
        "### **Objective:**\n",
        "To analyze the relationship between population size and key demographic indicators (e.g., growth rate, density, and urban population percentage) for all countries, using the most recent data available on the Worldometers website.\n",
        "\n",
        "### **Data Needed:**\n",
        "To answer this research question, we need the following types of data:\n",
        "\n",
        "#### **Country Information:**\n",
        "- **Fields:** Country name, population size, yearly growth rate, population density (people per square km), urban population percentage, and country rank.\n",
        "- **Source:** Worldometers website ([https://www.worldometers.info/world-population/population-by-country/](https://www.worldometers.info/world-population/population-by-country/)).\n",
        "\n",
        "#### **Other Indicators (Optional):**\n",
        "- **Fields:** Median age, fertility rate, migrants, and life expectancy.\n",
        "- **Source:** The same Worldometers page may contain these fields if they are present in the table.\n",
        "\n",
        "### **Amount of Data Needed:**\n",
        "- All available countries listed on the Worldometers page (approximately 200+ countries).\n",
        "- Full set of demographic indicators for each country (as listed on the webpage).\n",
        "\n",
        "### **Steps for Collecting and Saving the Data:**\n",
        "\n",
        "#### **Collect Country Data:**\n",
        "- Use Selenium or BeautifulSoup to extract data from the table on the Worldometers page.\n",
        "- Parse the table to collect fields like population size, growth rate, density, and urban population percentage.\n",
        "\n",
        "#### **Store the Data:**\n",
        "- Use pandas to structure the collected data in a DataFrame format.\n",
        "- Save the cleaned and structured data into a CSV file or a database for further analysis.\n"
      ],
      "metadata": {
        "id": "_YOOHA6nu-h-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here"
      ],
      "metadata": {
        "id": "4XvRknixTh1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install pandas\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_world_population_data(url):\n",
        "    # Send a GET request to the webpage\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the webpage content\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find the table containing population data\n",
        "    table = soup.find('table', {'id': 'example2'})\n",
        "\n",
        "    # Extract table headers\n",
        "    headers = [header.text.strip() for header in table.find_all('th')]\n",
        "\n",
        "    # Extract table rows\n",
        "    rows = []\n",
        "    for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
        "        cols = row.find_all('td')\n",
        "        row_data = [col.text.strip() for col in cols]\n",
        "        rows.append(row_data)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(rows, columns=headers)\n",
        "\n",
        "    return df\n",
        "\n",
        "# URL of the Worldometers page containing the population data\n",
        "url = 'https://www.worldometers.info/world-population/population-by-country/'\n",
        "df = scrape_world_population_data(url)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "df.to_csv('world_population_by_country.csv', index=False)\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06AwQBraBPkG",
        "outputId": "94a92293-137a-4c35-81ed-1135cf7b8f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "   # Country (or dependency) Population (2024) Yearly Change  Net Change  \\\n",
            "0  1                   India     1,450,935,791        0.89 %  12,866,195   \n",
            "1  2                   China     1,419,321,278       -0.23 %  -3,263,655   \n",
            "2  3           United States       345,426,571        0.57 %   1,949,236   \n",
            "3  4               Indonesia       283,487,931        0.82 %   2,297,864   \n",
            "4  5                Pakistan       251,269,164        1.52 %   3,764,669   \n",
            "\n",
            "  Density (P/Km²) Land Area (Km²) Migrants (net) Fert. Rate Med. Age  \\\n",
            "0             488       2,973,190       -630,830        2.0       28   \n",
            "1             151       9,388,211       -318,992        1.0       40   \n",
            "2              38       9,147,420      1,286,132        1.6       38   \n",
            "3             156       1,811,570        -38,469        2.1       30   \n",
            "4             326         770,880     -1,401,173        3.5       20   \n",
            "\n",
            "  Urban Pop % World Share  \n",
            "0        37 %     17.78 %  \n",
            "1        66 %     17.39 %  \n",
            "2        82 %      4.23 %  \n",
            "3        59 %      3.47 %  \n",
            "4        34 %      3.08 %  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "def collect_articles_semantic_scholar(query, st_year, ed_year, no_of_articles):\n",
        "    base_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "    headers = {\"Accept\": \"application/json\"}\n",
        "    articles = []\n",
        "    offset = 0\n",
        "    limit = 100\n",
        "\n",
        "    while len(articles) < no_of_articles:\n",
        "        params = {\n",
        "            \"query\": query,\n",
        "            \"offset\": offset,\n",
        "            \"limit\": min(limit, no_of_articles - len(articles)),\n",
        "            \"fields\": \"title,venue,year,authors,abstract\",\n",
        "            \"yearFilter\": f\"{st_year}-{ed_year}\"\n",
        "        }\n",
        "\n",
        "        response = requests.get(base_url, headers=headers, params=params)\n",
        "\n",
        "#        if response.status_code == 429:\n",
        " #           print(\"Rate limit exceeded, sleeping for 60 seconds...\")\n",
        "  #          time.sleep(60)  # Sleep for 60 seconds before retrying\n",
        "   #         continue\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Error: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        data = response.json().get('data', [])\n",
        "        for i in data:\n",
        "            if len(articles) >= no_of_articles:\n",
        "                break\n",
        "\n",
        "            article = {\n",
        "                'title': i.get('title', 'No title'),\n",
        "                'venue': i.get('venue', 'No venue'),\n",
        "                'year': i.get('year', 'No year'),\n",
        "                'authors': ', '.join([author.get('name', 'Unknown') for author in i.get('authors', [])]),\n",
        "                'abstract': i.get('abstract', 'No abstract')\n",
        "            }\n",
        "            articles.append(article)\n",
        "\n",
        "        offset += limit\n",
        "\n",
        "    return articles\n",
        "\n",
        "\n",
        "query = \"XYZ\"\n",
        "st_year = 2014\n",
        "ed_year = 2024\n",
        "no_of_articles = 1000\n",
        "articles = collect_articles_semantic_scholar(query, st_year, ed_year, no_of_articles)\n",
        "\n",
        "df = pd.DataFrame(articles)\n",
        "df.to_csv('articles.csv', index=False)\n",
        "\n",
        "for i, article in enumerate(articles[:5], 1):\n",
        "    print(f\"Article {i}:\")\n",
        "    print(f\"Title: {article['title']}\")\n",
        "    print(f\"Venue: {article['venue']}\")\n",
        "    print(f\"Year: {article['year']}\")\n",
        "    print(f\"Authors: {article['authors']}\")\n",
        "    print(f\"Abstract: {article['abstract']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjrqq-gWxuMR",
        "outputId": "c6b71198-a572-4eea-ffd4-b1e61bec4676"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU"
      },
      "outputs": [],
      "source": [
        "# write your answer here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWDcIz4hLYrW",
        "outputId": "e2ac22dd-0121-496f-b91f-84fbfa91abfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.8.30)\n",
            "Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "# Reddit API credentials\n",
        "client_id = 'wXw8RAzd_hkSmKt8j2Pu2w'\n",
        "client_secret = 'svWmPzQaiaQ_V8x12H1W56ZRzn8dWQ'\n",
        "user_agent = 'EmbarrassedCover2033'"
      ],
      "metadata": {
        "id": "u0OwYpU_LdjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate to Reddit using PRAW\n",
        "reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)\n",
        "\n",
        "# Define the subreddit and search parameters\n",
        "subreddit_name = 'stocks'\n",
        "search_query = 'Apple'\n",
        "\n",
        "# Function to collect Reddit posts\n",
        "def collect_reddit_posts(subreddit_name, search_query):\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    posts_data = []\n",
        "\n",
        "    # Search for posts in the subreddit\n",
        "    for submission in subreddit.search(search_query, limit=100):\n",
        "        post_id = submission.id\n",
        "        title = submission.title\n",
        "        author = submission.author.name if submission.author else 'N/A'\n",
        "        created_at = submission.created_utc\n",
        "        num_comments = submission.num_comments\n",
        "        score = submission.score\n",
        "\n",
        "        posts_data.append([post_id, title, author, created_at, num_comments, score])\n",
        "\n",
        "    return posts_data\n",
        "\n",
        "# Fetch and save Reddit posts data\n",
        "posts_data = collect_reddit_posts(subreddit_name, search_query)\n",
        "posts_df = pd.DataFrame(posts_data, columns=['Post ID', 'Title', 'Author', 'Created At', 'Number of Comments', 'Score'])\n",
        "posts_df.to_csv('reddit_posts_data.csv', index=False)\n",
        "\n",
        "print(\"Data collection completed and saved to reddit_posts_data.csv.\")\n",
        "print(posts_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylZBCqLJL8e-",
        "outputId": "64a210e7-5d8f-44f3-ecca-0c2010c15868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data collection completed and saved to reddit_posts_data.csv.\n",
            "   Post ID                                              Title  \\\n",
            "0  1ej2a5e  Warren Buffett’s Berkshire Hathaway sold nearl...   \n",
            "1  1bk7xgd                DOJ sues Apple over iPhone monopoly   \n",
            "2  1ciq1fw  Apple announces largest-ever $110 billion shar...   \n",
            "3  1b6agqo  Apple hit with more than $1.95 billion EU anti...   \n",
            "4  1fdcraa  Apple loses EU court battle over 13 billion eu...   \n",
            "\n",
            "            Author    Created At  Number of Comments  Score  \n",
            "0  themagicalpanda  1.722688e+09                 543   3422  \n",
            "1        Puginator  1.711032e+09                 912   2723  \n",
            "2        Puginator  1.714682e+09                 524   2993  \n",
            "3        Puginator  1.709558e+09                 414   1685  \n",
            "4        Puginator  1.725956e+09                 261    862  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}